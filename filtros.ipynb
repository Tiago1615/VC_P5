{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Santiago\\anaconda3\\envs\\P5_VC\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Paquetes necesarios\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_image_alpha(img, overlay, position, alpha_mask):\n",
    "    \"\"\"Superpone una imagen `overlay` en la posición especificada sobre la `img`.\"\"\"\n",
    "    x, y = position\n",
    "    h, w = overlay.shape[:2]\n",
    "\n",
    "    # Asegurar que el overlay encaje dentro de los límites de `img`\n",
    "    if y < 0:  # Si el overlay se sale por arriba, recortamos\n",
    "        overlay = overlay[-y:, :, :]\n",
    "        alpha_mask = alpha_mask[-y:, :]\n",
    "        y = 0\n",
    "    if x < 0:  # Si el overlay se sale por la izquierda, recortamos\n",
    "        overlay = overlay[:, -x:, :]\n",
    "        alpha_mask = alpha_mask[:, -x:]\n",
    "        x = 0\n",
    "    if y + h > img.shape[0]:  # Si el overlay se sale por abajo, recortamos\n",
    "        h = img.shape[0] - y\n",
    "        overlay = overlay[:h, :, :]\n",
    "        alpha_mask = alpha_mask[:h, :]\n",
    "    if x + w > img.shape[1]:  # Si el overlay se sale por la derecha, recortamos\n",
    "        w = img.shape[1] - x\n",
    "        overlay = overlay[:, :w, :]\n",
    "        alpha_mask = alpha_mask[:, :w]\n",
    "\n",
    "    # Corta la región donde se colocará el overlay\n",
    "    region = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Asegura que alpha_mask esté en el rango correcto\n",
    "    alpha_mask = alpha_mask.astype(float) / 255.0\n",
    "\n",
    "    # Combina el overlay con la región del frame usando el canal alfa\n",
    "    for c in range(3):  # Para cada canal (RGB)\n",
    "        region[:, :, c] = (1.0 - alpha_mask) * region[:, :, c] + alpha_mask * overlay[:, :, c]\n",
    "\n",
    "    img[y:y+h, x:x+w] = region\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en análisis de emociones: operands could not be broadcast together with shapes (400,358) (480,358) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (403,358) (480,358) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (393,346) (480,346) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (391,342) (480,342) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (394,340) (480,340) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (395,336) (480,336) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (389,344) (480,344) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (394,346) (480,346) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (396,348) (480,348) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (392,348) (480,348) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (397,350) (480,350) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (393,352) (480,352) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (393,352) (480,352) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (393,352) (480,352) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (396,340) (480,340) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (396,340) (480,340) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (393,344) (480,344) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (394,348) (480,348) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (392,352) (480,352) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (394,348) (480,348) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (393,348) (480,348) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (394,344) (480,344) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (393,342) (480,342) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (395,348) (480,348) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (396,344) (480,344) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (396,346) (480,346) \n",
      "Error en análisis de emociones: operands could not be broadcast together with shapes (396,346) (480,346) \n"
     ]
    }
   ],
   "source": [
    "# Configuración de la cámara y el modelo DNN para detección de rostros\n",
    "dnn_model = \"deploy.prototxt.txt\"\n",
    "dnn_weights = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(dnn_model, dnn_weights)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Cargar gráficos del cabello y aura (asegúrate de que son PNG con canal alfa)\n",
    "hair = cv2.imread('pelo_super_saiyan.png', cv2.IMREAD_UNCHANGED)\n",
    "aura = cv2.imread('aura_super_saiyan.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Procesar el video en tiempo real\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detección de rostros\n",
    "    h, w = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104.0, 177.0, 123.0], False, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Para cada cara detectada\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x2, y2) = box.astype(\"int\")\n",
    "            face_width = x2 - x\n",
    "            face_height = y2 - y\n",
    "\n",
    "            # Detectar emoción con DeepFace\n",
    "            try:\n",
    "                face_img = frame[y:y2, x:x2]  # Extrae el rostro detectado\n",
    "                analysis = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)\n",
    "                emotion = analysis[0]['dominant_emotion']\n",
    "\n",
    "                # Activar efectos si la emoción es enfado\n",
    "                if emotion == 'angry':\n",
    "                    # Redimensionar gráficos de cabello y aura\n",
    "                    hair_resized = cv2.resize(hair, (face_width, face_height))\n",
    "                    aura_resized = cv2.resize(aura, (face_width * 2, face_height * 2))  # Aura puede ser más grande\n",
    "\n",
    "                    # Extraer canal alfa como máscara\n",
    "                    hair_alpha = hair_resized[:, :, 3]  # Canal alfa del cabello\n",
    "                    aura_alpha = aura_resized[:, :, 3]  # Canal alfa del aura\n",
    "\n",
    "                    # Superponer gráficos sobre el frame en posiciones específicas\n",
    "                    frame = overlay_image_alpha(frame, hair_resized[:, :, :3], (x, y - face_height // 2), hair_alpha)\n",
    "                    frame = overlay_image_alpha(frame, aura_resized[:, :, :3], (x - face_width // 2, y - face_height), aura_alpha)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error en análisis de emociones:\", e)\n",
    "\n",
    "    cv2.imshow('Filtro Saiyan', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Salir con ESC\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar ventanas\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P5_VC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
